# RL-Project
# Reinforcement Learning for Digital Advertising Optimization

ë³¸ í”„ë¡œì íŠ¸ëŠ” **ë””ì§€í„¸ ê´‘ê³  ì‹œìŠ¤í…œì—ì„œ ì–´ë–¤ ì‚¬ìš©ìì—ê²Œ ê´‘ê³ (treatment)ë¥¼ ë…¸ì¶œí• ì§€ ê²°ì •í•˜ëŠ” ì •ì±…(policy)ì„ ê°•í™”í•™ìŠµìœ¼ë¡œ í•™ìŠµ**í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.  
ëª©í‘œëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë™ì‹œì— ìµœì í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

- **ì „í™˜(conversion) ë° ë°©ë¬¸(visit) ê·¹ëŒ€í™”**
- **ê´‘ê³  ë¹„ìš©(CPM, CPC ë“±) ìµœì†Œí™”**

ì´ë¥¼ ìœ„í•´ Policy Gradient(PG), Deep Q-Network(DQN), Advantage Actor-Critic(A2C) 3ê°€ì§€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ë™ì¼ í™˜ê²½ì—ì„œ ë¹„êµ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤.

---

## ğŸš€ Project Overview

### âœ¨ Problem Definition
- ê´‘ê³  ë…¸ì¶œì„ í• ì§€(treat=1) í•˜ì§€ ì•Šì„ì§€(treat=0)ë¥¼ ê²°ì •í•˜ëŠ” **two-action RL ë¬¸ì œ**
- ì‚¬ìš©ìë³„ featureë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´‘ê³  íš¨ê³¼(visit, conversion)ë¥¼ ìµœëŒ€í™”
- reward êµ¬ì„± ìš”ì†Œ:
  - ê´‘ê³ ë¥¼ ë…¸ì¶œí•˜ë©´ CPM ë¹„ìš© ì°¨ê°  
  - í´ë¦­/ë°©ë¬¸ ë°œìƒ ì‹œ CPC ë¹„ìš© ë°˜ì˜  
  - ì „í™˜ ë°œìƒ ì‹œ ì¶”ê°€ reward  
- ê²°ê³¼ì ìœ¼ë¡œ **ì¥ê¸°ì  ê¸°ëŒ€ ì´ìµì„ ìµœëŒ€í™”í•˜ëŠ” ì •ì±…ì„ í•™ìŠµ**

---

## ğŸ¤– Algorithms Implemented

### 1. **Policy Gradient (PG)**
- í™•ë¥ ì  ì •ì±…ì„ ì§ì ‘ ìµœì í™”
- baseline ëŒ€ë¹„ ê°€ì¥ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ê³  ë†’ì€ validation reward ë‹¬ì„±

### 2. **Deep Q-Network (DQN)**
- action-value ê¸°ë°˜ ëª¨ë¸
- replay buffer + Îµ-greedy exploration

### 3. **Advantage Actor-Critic (A2C)**
- PG + Value function ê²°í•©
- ì•ˆì •ì  í•™ìŠµ, ë§ˆì§€ë§‰ ì„±ëŠ¥ ìµœê³ 

---

## ğŸ— Environment & Reward Design

### âœ” State
- ì‚¬ìš©ì feature vector (ë³€í™˜ ê°€ëŠ¥)

### âœ” Action
- `0`: ê´‘ê³  ë¯¸ë…¸ì¶œ (No-Ad)  
- `1`: ê´‘ê³  ë…¸ì¶œ (Treat)

### âœ” Reward
- **treat=1** ì¼ ë•Œ CPM ë¹„ìš© ì°¨ê°  
- **visit=1** ë°œìƒ ì‹œ CPC ë¹„ìš© ì°¨ê°  
- **conversion=1** ë°œìƒ ì‹œ í° reward ë¶€ì—¬  
- ê¸°ì¡´ reward ëŒ€ë¹„ ê°œì„ ëœ ë²„ì „ â†’ í•™ìŠµ ì•ˆì •ì„± ë° ì„±ëŠ¥ í–¥ìƒ

---

## ğŸ“ˆ Training Results

### 1ï¸âƒ£ **Policy Gradient (PG)**  
![PG Curve](./plots/pg_curve.png)

- reward varianceê°€ ìˆì§€ë§Œ ê¾¸ì¤€íˆ ì¦ê°€
- validation ê¸°ì¤€ ì•½ **0.075~0.085** ìˆ˜ì¤€ ë„ë‹¬

---

### 2ï¸âƒ£ **DQN**
![DQN Curve](./plots/dqn_curve.png)

- exploration ì˜í–¥ìœ¼ë¡œ ì´ˆê¸° ì§„ë™ì´ í¬ì§€ë§Œ  
- ì „ë°˜ì ìœ¼ë¡œ **0.04~0.06** validation reward í™•ë³´  
- baseline ëŒ€ë¹„ í™•ì‹¤í•œ ê°œì„ 

---

### 3ï¸âƒ£ **A2C**
![A2C Curve](./plots/a2c_curve.png)

- ì•ˆì •ì ìœ¼ë¡œ reward ìƒìŠ¹  
- ìµœì¢…ì ìœ¼ë¡œ **PGì™€ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë†’ì€ validation reward** ë„ë‹¬

---

## ğŸ§ª Final Comparison (Baseline vs RL Models)

ê°•í™”í•™ìŠµ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë‹¤ìŒ 3ê°€ì§€ baselineê³¼ ë¹„êµ:

- **No-Ad**: í•­ìƒ ê´‘ê³  ì•ˆ í•¨  
- **Always-Treat**: ëª¨ë“  ì‚¬ìš©ìì—ê²Œ ê´‘ê³   
- **Random**: ì„ì˜ë¡œ ë…¸ì¶œ  

### ğŸ”¹ ê°œì„ ëœ reward ì„¤ê³„ ì´í›„ ê²°ê³¼

![Final Comparison 1](./plots/final_comparison_reward_new1.png)

- **A2C(best) â‰ˆ PG(best) > DQN(best)**  
- ê¸°ì¡´ baseline ëŒ€ë¹„ **2~10ë°° ë†’ì€ reward**

---

### ğŸ”¹ ê¸°ì¡´ reward ëŒ€ë¹„ ì„±ëŠ¥ ì°¨ì´
![Final Comparison 2](./plots/final_comparison_reward_new2.png)

- reward ê°œì„  í›„ ëª¨ë“  RL ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì´ í¬ê²Œ ìƒìŠ¹  
- íŠ¹íˆ PG/A2CëŠ” ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ ëª¨ë‘ ê°œì„ ë¨

---

## ğŸ§  Discussion

### âœ” ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì´ ê°€ì¥ ì¢‹ì•˜ëŠ”ê°€?
- **A2Cê°€ ê°€ì¥ ì•ˆì •ì ì´ë©° ìµœì¢… reward ìµœê³ **
- PGëŠ” ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ë†’ì€ rewardë¥¼ ë³´ì„
- DQNì€ ë³€ë™ì„±ì€ ìˆìœ¼ë‚˜ baseline ëŒ€ë¹„ ê°•ë ¥í•œ ì„±ëŠ¥

### âœ” Reward ì„¤ê³„ê°€ ì¤‘ìš”í•œ ì´ìœ 
- ì´ˆê¸° reward ì„¤ê³„ì—ì„œëŠ” treat ì„ íƒì˜ ê·¼ê±°ê°€ ì•½í–ˆìœ¼ë‚˜  
- ê´‘ê³  ë¹„ìš©Â·ë°©ë¬¸Â·ì „í™˜ì„ ë°˜ì˜í•œ rewardë¡œ ë°”ê¾¼ ì´í›„  
  â†’ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ rewardê°€ í¬ê²Œ ê°œì„ ë¨

---

## â–¶ï¸ How to Run

```bash
pip install -r requirements.txt
python train_pg.py
python train_dqn.py
python train_a2c.py
